{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FITS Headers in MongoDB\n",
    "\n",
    "We continue our exploration of loading astronomical data into a MongoDB database. In this instance, we'll look at FITS Headers and how we can store them inside MongoDB. This is meant to accompany the blog post at [Strakul's Thoughts](https://strakul.blogspot.com/2019/06/data-science-astronomy-fits-headers-in.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining a FITS header\n",
    "\n",
    "First off, we'll explore an ordinary fits file with `astropy`. In particular, we'll use some of the test files provided by the package itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "\n",
    "fits_image_filename = fits.util.get_testdata_filepath('test0.fits')\n",
    "\n",
    "hdul = fits.open(fits_image_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine what header information is present for this file. For clarity, I only display a few of the cards and I'll only  be considering the primary header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/drodriguez/anaconda3/lib/python3.7/site-packages/astropy/io/fits/tests/data/test0.fits\n",
      "No.    Name      Ver    Type      Cards   Dimensions   Format\n",
      "  0  PRIMARY       1 PrimaryHDU     138   ()      \n",
      "  1  SCI           1 ImageHDU        61   (40, 40)   int16   \n",
      "  2  SCI           2 ImageHDU        61   (40, 40)   int16   \n",
      "  3  SCI           3 ImageHDU        61   (40, 40)   int16   \n",
      "  4  SCI           4 ImageHDU        61   (40, 40)   int16   \n"
     ]
    }
   ],
   "source": [
    "hdul.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SIMPLE  =                    T / file does conform to FITS standard             \n",
       "BITPIX  =                   16 / number of bits per data pixel                  \n",
       "NAXIS   =                    0 / number of data axes                            \n",
       "EXTEND  =                    T / FITS dataset may contain extensions            \n",
       "GROUPS  =                    F / data has groups                                \n",
       "NEXTEND =                    4 / Number of standard extensions                  \n",
       "BSCALE  =           1.000000E0 / REAL = TAPE*BSCALE + BZERO                     \n",
       "BZERO   =           3.276800E4 /                                                \n",
       "ORIGIN  = 'NOAO-IRAF FITS Image Kernel Aug 1 1997' / FITS file originator       \n",
       "DATE    = '01/04/99  '         / Date FITS file was generated                   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdul[0].header[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hdul[0].header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in our prior exercise, we'll want to convert this output to JSON in some way. We'll make use of the built-in method `__dict__` to express the header as a dictionary and, in particular, store only the key `_cards` which stores the actual header information as a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SIMPLE', True, 'file does conform to FITS standard'),\n",
       " ('BITPIX', 16, 'number of bits per data pixel'),\n",
       " ('NAXIS', 0, 'number of data axes'),\n",
       " ('EXTEND', True, 'FITS dataset may contain extensions'),\n",
       " ('GROUPS', False, 'data has groups'),\n",
       " ('NEXTEND', 4, 'Number of standard extensions'),\n",
       " ('BSCALE', 1.0, 'REAL = TAPE*BSCALE + BZERO'),\n",
       " ('BZERO', 32768.0, ''),\n",
       " ('ORIGIN', 'NOAO-IRAF FITS Image Kernel Aug 1 1997', 'FITS file originator'),\n",
       " ('DATE', '01/04/99', 'Date FITS file was generated')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.__dict__['_cards'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to extract the FITS header, we'll loop over the cards and store them in a dictionary which we'll convert to JSON. I decided to store only they key and value, but provide some commented out code to how one can also store the comment field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"SIMPLE\": true,\n",
      "    \"BITPIX\": 16,\n",
      "    \"NAXIS\": 0,\n",
      "    \"EXTEND\": true,\n",
      "    \"GROUPS\": false,\n",
      "    \"NEXTEND\": 4,\n",
      "    \"BSCALE\": 1.0,\n",
      "    \"BZERO\": 32768.0,\n",
      "    \"ORIGIN\": \"NOAO-IRAF FITS Image Kernel Aug 1 1997\",\n",
      "    \"DATE\": \"01/04/99\",\n",
      "    \"IRAF-TLM\": \"xxx\",\n",
      "    \"INSTRUME\": \"WFPC2\",\n",
      "    \"ROOTNAME\": \"U2EQ0201T\",\n",
      "    \"FILETYPE\": \"SCI\",\n",
      "    \"MODE\": \"AREA\",\n",
      "    \"SERIALS\": \"OFF\",\n",
      "    \"IMAGETYP\": \"EXT\",\n",
      "    \"CDBSFILE\": \"NO\",\n",
      "    \"PKTFMT\": 96,\n",
      "    \"FILTNAM1\": \"F673N\",\n",
      "    \"FILTNAM2\": \"\",\n",
      "    \"FILTER1\": 33,\n",
      "    \"FILTER2\": 0,\n",
      "    \"FILTROT\": 0.0,\n",
      "    \"LRFWAVE\": 0.0,\n",
      "    \"UCH1CJTM\": -88.3486,\n",
      "    \"UCH2CJTM\": -88.8073,\n",
      "    \"UCH3CJTM\": -88.3945,\n",
      "    \"UCH4CJTM\": -88.9041,\n",
      "    \"UBAY3TMP\": 14.5969,\n",
      "    \"KSPOTS\": \"OFF\",\n",
      "    \"SHUTTER\": \"B\",\n",
      "    \"ATODGAIN\": 7.0,\n",
      "    \"MASKCORR\": \"PERFORM\",\n",
      "    \"ATODCORR\": \"PERFORM\",\n",
      "    \"BLEVCORR\": \"PERFORM\",\n",
      "    \"BIASCORR\": \"PERFORM\",\n",
      "    \"DARKCORR\": \"OMIT\",\n",
      "    \"FLATCORR\": \"PERFORM\",\n",
      "    \"SHADCORR\": \"PERFORM\",\n",
      "    \"DOSATMAP\": \"OMIT\",\n",
      "    \"DOPHOTOM\": \"PERFORM\",\n",
      "    \"DOHISTOS\": \"OMIT\",\n",
      "    \"OUTDTYPE\": \"REAL\",\n",
      "    \"MASKFILE\": \"uref$fan15478u.r0h\",\n",
      "    \"ATODFILE\": \"uref$e1b09594u.r1h\",\n",
      "    \"BLEVFILE\": \"ucal$u2eq0201t.x0h\",\n",
      "    \"BLEVDFIL\": \"ucal$u2eq0201t.q1h\",\n",
      "    \"BIASFILE\": \"uref$e6o0937du.r2h\",\n",
      "    \"BIASDFIL\": \"uref$e6o0937du.b2h\",\n",
      "    \"DARKFILE\": \"uref$e6o09384u.r3h\",\n",
      "    \"DARKDFIL\": \"uref$e6o09384u.b3h\",\n",
      "    \"FLATFILE\": \"uref$e1c1404ju.r4h\",\n",
      "    \"FLATDFIL\": \"uref$e1c1404ju.b4h\",\n",
      "    \"SHADFILE\": \"uref$e6o09405u.r5h\",\n",
      "    \"PHOTTAB\": \"\",\n",
      "    \"GRAPHTAB\": \"mtab$f7d1401pm.tmg\",\n",
      "    \"COMPTAB\": \"mtab$f7j1535pm.tmc\",\n",
      "    \"SATURATE\": 4095,\n",
      "    \"USCALE\": 1.0,\n",
      "    \"UZERO\": 0.0,\n",
      "    \"READTIME\": 151,\n",
      "    \"PA_V3\": 292.387786865,\n",
      "    \"RA_SUN\": 56.2731208362,\n",
      "    \"DEC_SUN\": 19.8289107164,\n",
      "    \"EQNX_SUN\": 2000.0,\n",
      "    \"MTFLAG\": true,\n",
      "    \"EQRADTRG\": 0.0,\n",
      "    \"FLATNTRG\": 0.0,\n",
      "    \"NPDECTRG\": 0.0,\n",
      "    \"NPRATRG\": 0.0,\n",
      "    \"ROTRTTRG\": 0.0,\n",
      "    \"LONGPMER\": 0.0,\n",
      "    \"EPLONGPM\": 0.0,\n",
      "    \"SURFLATD\": 0.0,\n",
      "    \"SURFLONG\": 0.0,\n",
      "    \"SURFALTD\": 0.0,\n",
      "    \"PODPSFF\": false,\n",
      "    \"STDCFFF\": false,\n",
      "    \"STDCFFP\": \"0x5569\",\n",
      "    \"RSDPFILL\": -100,\n",
      "    \"UEXPODUR\": 0,\n",
      "    \"NSHUTA17\": 0,\n",
      "    \"DARKTIME\": 0.23,\n",
      "    \"UEXPOTIM\": 20272,\n",
      "    \"PSTRTIME\": \"1994.139:15:41:39\",\n",
      "    \"PSTPTIME\": \"1994.139:16:14:39\",\n",
      "    \"ORIENTAT\": 157.076,\n",
      "    \"SUNANGLE\": 158.939133,\n",
      "    \"MOONANGL\": 54.995346,\n",
      "    \"SUN_ALT\": -11.552458,\n",
      "    \"FGSLOCK\": \"FINE\",\n",
      "    \"DATE-OBS\": \"19/05/94\",\n",
      "    \"TIME-OBS\": \"15:41:16\",\n",
      "    \"EXPSTART\": 49491.65366175,\n",
      "    \"EXPEND\": 49491.65366441,\n",
      "    \"EXPTIME\": 0.23,\n",
      "    \"EXPFLAG\": \"NORMAL\",\n",
      "    \"FILENAME\": \"vtest3.fits\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = {}\n",
    "for i, c in enumerate(h.__dict__['_cards']):\n",
    "    if c[0] == '': continue\n",
    "    data[c[0]] = c[1]\n",
    "    \n",
    "    # An alternative to store comments in addition to values\n",
    "    #data[c[0]] = {'value': c[1], 'comment': c[2]}\n",
    "    \n",
    "print(json.dumps(data, default=lambda x: x.__dict__, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple database load\n",
    "\n",
    "This is a fairly simple document, so inserting into MongoDB should not be any issue. As before, we'll establish a connection to our locally running server and create a new collection, called 'fits', to store this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient()  # default connection (ie, local)\n",
    "\n",
    "db = client['test']  # database\n",
    "collection = db.fits  # collection; can also call as db['fits']\n",
    "collection.drop()  # drop collection, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5cf02ccc5abbd9a33e10c74e'), 'SIMPLE': True, 'BITPIX': 16, 'NAXIS': 0, 'EXTEND': True, 'GROUPS': False, 'NEXTEND': 4, 'BSCALE': 1.0, 'BZERO': 32768.0, 'ORIGIN': 'NOAO-IRAF FITS Image Kernel Aug 1 1997', 'DATE': '01/04/99', 'IRAF-TLM': 'xxx', 'INSTRUME': 'WFPC2', 'ROOTNAME': 'U2EQ0201T', 'FILETYPE': 'SCI', 'MODE': 'AREA', 'SERIALS': 'OFF', 'IMAGETYP': 'EXT', 'CDBSFILE': 'NO', 'PKTFMT': 96, 'FILTNAM1': 'F673N', 'FILTNAM2': '', 'FILTER1': 33, 'FILTER2': 0, 'FILTROT': 0.0, 'LRFWAVE': 0.0, 'UCH1CJTM': -88.3486, 'UCH2CJTM': -88.8073, 'UCH3CJTM': -88.3945, 'UCH4CJTM': -88.9041, 'UBAY3TMP': 14.5969, 'KSPOTS': 'OFF', 'SHUTTER': 'B', 'ATODGAIN': 7.0, 'MASKCORR': 'PERFORM', 'ATODCORR': 'PERFORM', 'BLEVCORR': 'PERFORM', 'BIASCORR': 'PERFORM', 'DARKCORR': 'OMIT', 'FLATCORR': 'PERFORM', 'SHADCORR': 'PERFORM', 'DOSATMAP': 'OMIT', 'DOPHOTOM': 'PERFORM', 'DOHISTOS': 'OMIT', 'OUTDTYPE': 'REAL', 'MASKFILE': 'uref$fan15478u.r0h', 'ATODFILE': 'uref$e1b09594u.r1h', 'BLEVFILE': 'ucal$u2eq0201t.x0h', 'BLEVDFIL': 'ucal$u2eq0201t.q1h', 'BIASFILE': 'uref$e6o0937du.r2h', 'BIASDFIL': 'uref$e6o0937du.b2h', 'DARKFILE': 'uref$e6o09384u.r3h', 'DARKDFIL': 'uref$e6o09384u.b3h', 'FLATFILE': 'uref$e1c1404ju.r4h', 'FLATDFIL': 'uref$e1c1404ju.b4h', 'SHADFILE': 'uref$e6o09405u.r5h', 'PHOTTAB': '', 'GRAPHTAB': 'mtab$f7d1401pm.tmg', 'COMPTAB': 'mtab$f7j1535pm.tmc', 'SATURATE': 4095, 'USCALE': 1.0, 'UZERO': 0.0, 'READTIME': 151, 'PA_V3': 292.387786865, 'RA_SUN': 56.2731208362, 'DEC_SUN': 19.8289107164, 'EQNX_SUN': 2000.0, 'MTFLAG': True, 'EQRADTRG': 0.0, 'FLATNTRG': 0.0, 'NPDECTRG': 0.0, 'NPRATRG': 0.0, 'ROTRTTRG': 0.0, 'LONGPMER': 0.0, 'EPLONGPM': 0.0, 'SURFLATD': 0.0, 'SURFLONG': 0.0, 'SURFALTD': 0.0, 'PODPSFF': False, 'STDCFFF': False, 'STDCFFP': '0x5569', 'RSDPFILL': -100, 'UEXPODUR': 0, 'NSHUTA17': 0, 'DARKTIME': 0.23, 'UEXPOTIM': 20272, 'PSTRTIME': '1994.139:15:41:39', 'PSTPTIME': '1994.139:16:14:39', 'ORIENTAT': 157.076, 'SUNANGLE': 158.939133, 'MOONANGL': 54.995346, 'SUN_ALT': -11.552458, 'FGSLOCK': 'FINE', 'DATE-OBS': '19/05/94', 'TIME-OBS': '15:41:16', 'EXPSTART': 49491.65366175, 'EXPEND': 49491.65366441, 'EXPTIME': 0.23, 'EXPFLAG': 'NORMAL', 'FILENAME': 'vtest3.fits'}\n"
     ]
    }
   ],
   "source": [
    "json_data = json.loads(json.dumps(data, default=lambda x: x.__dict__, indent=4))\n",
    "result = collection.insert_one(json_data)\n",
    "\n",
    "# Quick check to confirm load\n",
    "cursor = collection.find({'FILENAME': 'vtest3.fits'})\n",
    "for doc in cursor:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a wrapper to handle inserts\n",
    "\n",
    "Now, let's put all of those steps together into something we can run more easily. Below I've created a class called `FitsWrapper`, which takes a filename, gets the primary header, constructs the dictionary out of it, and has methods to both produce the JSON output and to store it in a collection the user defines. With this one class, I can do all of the steps above to quickly load FITS headers into my database. \n",
    "Note that I concatenate header keywords in the event multiple keywords are present, as is commonly the case with 'COMMENT'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from astropy.io import fits\n",
    "\n",
    "\n",
    "class FitsWrapper:\n",
    "    def __init__(self, fits_filename):\n",
    "        hdul = fits.open(fits_filename)\n",
    "        self.h = hdul[0].header\n",
    "        hdul.close()\n",
    "        \n",
    "        self.data = {}\n",
    "        for i, c in enumerate(self.h.__dict__['_cards']):\n",
    "            if c[0] == '': continue\n",
    "                \n",
    "            # Append to existing cards\n",
    "            if self.data.get(c[0], None) is None:\n",
    "                self.data[c[0]] = c[1]\n",
    "            else:\n",
    "                self.data[c[0]] = self.data[c[0]] + ' ' + c[1]\n",
    "            \n",
    "        if self.data.get('FILENAME', None) is None:\n",
    "            self.data['FILENAME'] = os.path.basename(fits_filename)\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self.data, default=lambda x: x.__dict__, indent=4)\n",
    "        \n",
    "    def load_mongodb(self, collection, verbose=False):\n",
    "        json_data = json.loads(self.to_json())\n",
    "        \n",
    "        # This uses replace_one to replace any existing document that matches the filter. \n",
    "        # If none is matched, upsert=True creates a new document.\n",
    "        result = collection.replace_one(filter={'FILENAME': self.data['FILENAME']}, replacement=json_data, upsert=True)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Modified: ', result.modified_count)\n",
    "            print('Insert ID: ', result.upserted_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified:  1\n",
      "Insert ID:  None\n"
     ]
    }
   ],
   "source": [
    "fits_image_filename = fits.util.get_testdata_filepath('test0.fits')\n",
    "fw = FitsWrapper(fits_image_filename)\n",
    "fw.load_mongodb(collection, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's that simple.\n",
    "\n",
    "Now, if you look closely at the `load_mongodb` method, you'll notice I did not use `insert_one` as before, but instead `replace_one`. This is because I wanted to do something different. This method takes a filter and performs a search on the database for all documents that satisfy it, then it replaces them with the supplied JSON information. If no match is found and upsert is set to True, it creates that new document. This allows me to insert and update documents very easily and avoid duplication. By setting `verbose=True` in the call to `load_mongodb`, I output some debug statements showing that I modified 1 document and inserted no new ones.\n",
    "\n",
    "In this particular example, I'm querying by the FITS FILENAME, which I assume to be unique, but if it isn't, or you want to query for something else, it's straightforward to update the filter setting in that method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading more data\n",
    "\n",
    "Now, let's go ahead and load up some more data. The text below contains links of FITS files drawn from the BDNYC database of brown dwarfs. These are all online and easily accessible though our interface at http://database.bdnyc.org. I also added a few from the [Astropy Tutorials](http://learn.astropy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_links = \"\"\"https://s3.amazonaws.com/bdnyc/nir_spectra/spex_prism_1254-0122_030522.fits\n",
    "https://s3.amazonaws.com/bdnyc/U11122_1305-2541_kelu1_lris.fits\n",
    "https://s3.amazonaws.com/bdnyc/L2_Kelu-1AB.fits\n",
    "https://s3.amazonaws.com/bdnyc/SpeX/IRTF%20Library%20%28Prism%2BLXD%29/T4.5_2MASSJ0559-1404.fits\n",
    "http://data.astropy.org/tutorials/FITS-images/HorseHead.fits\n",
    "https://astropy.stsci.edu/data/tutorials/FITS-images/M13_blue_0001.fits\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in fits_links.split('\\n'):\n",
    "    try:\n",
    "        fw = FitsWrapper(f)\n",
    "        fw.load_mongodb(collection)\n",
    "    except Exception as e:\n",
    "        print(f'Error when loading {f}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many documents we've loaded thus far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents:  7\n"
     ]
    }
   ],
   "source": [
    "count = collection.count_documents({})\n",
    "print('Total documents: ', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not a lot, but it's a start. Now, something to bear in mind is that the metadata in this FITS files is very varied. They're coming from different telescopes, different instruments, and different processing pipelines. Searching through these documents can be tricky if you don't know the exact header keywords have been used. One thing we can do to alleviate this is to create a text index accross all fields. You can only have one text index in a MongoDB collection, I believe, but it can include any number of fields. If we include all of them, then no matter what the header keyword is, we can still end up finding the values of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text_fields'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collection.drop_index('text_fields')\n",
    "collection.create_index([('$**', pymongo.TEXT)], name='text_fields', background=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FILTNAM1': 'F673N', 'FILTNAM2': '', 'FILENAME': 'vtest3.fits'}\n",
      "{'FILTER': 'OG590', 'FILENAME': 'HorseHead.fits'}\n"
     ]
    }
   ],
   "source": [
    "cursor = collection.find({'$text': {'$search': 'OG590 F673N'}}, \n",
    "                         {'_id': 0, 'FILENAME': 1, 'FILTER': 1, 'FILTNAM1': 1, 'FILTNAM2': 1})\n",
    "for doc in cursor:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we've searched for either OG590 or F673N, two filters names (`$text` uses a logical OR search for the terms unless you enclose them in quotes, see the [documentation](https://docs.mongodb.com/manual/reference/operator/query/text/)). \n",
    "There are two documents that match this search, one for each filter. In one case, the filter keyword is FILTNAM1, but in the other its FILTER yet because the text index was created, any keyword with text content is included when doing a text search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "To wrap up, this has been a simple example of how you can populate a database with metadata from FITS headers. For simplicity, I only saved the primary header, but the example code here can be modified to run on any extension, or perhaps loop over all extensions. At that point you can decide if each extension is it's own single document or if they are embedded together to keep all the metadata together. \n",
    "\n",
    "Due to the varied nature of headers, a text index was created in the database to facilitate searches. If you have hundreds of FITS files as part of your work (I certainly did in my graduate school and postdoc days), you could use something like this to organize your data and make searching through it somewhat simpler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
